import time
from llama_index.llms import Ollama
from llama_index import VectorStoreIndex, ServiceContext, Document, SimpleDirectoryReader

llm = Ollama(model='llama2')

def stream_response(prompt):
    """
    Streams the completion of a prompt using a large language model (LLM).

    Parameters:
    - prompt (str): The prompt text to be completed by the language model.

    Yields:
    - delta (float): The delta value associated with each response generated
      by the language model in response to the provided prompt.

    Notes:
    - This function streams the completion of a given prompt using a language
      model (LLM) and yields the delta values associated with each response
      generated by the model.
    - The delta value represents a measure of difference or change associated
      with each response.
    - A small delay of 0.02 seconds is introduced between each iteration to
      control the streaming rate.

    Example:
    >>> for delta in stream_response("Generate a summary for a given text"):
    >>>     print(delta)
"""
    response = llm.stream_complete(prompt)
    for item in response:
        yield item.delta
        time.sleep(0.02)
